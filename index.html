<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>audio</title>
  <style>
    html{
      background: url("https://pic2.zhimg.com/v2-1b2dafff1629e5a1974d5565e1b14b91_r.jpg");
      background: url("https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1582437272354&di=b9e711ce5a5e9aacc48b46a109335711&imgtype=0&src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farchive%2F9b48e698aa775d5a2cbd2ffeb7a9626a8f0f7293.jpg");
      background-repeat: no-repeat;
      background-size: cover;
      height: 100%;
    }

    body {
      margin: 0;
    }

    input {
      width: 500px;
      border: solid 1px #ddd;
      border-radius: 5px;
      padding: 1.5em;
      outline: 0;
    }

  </style>
  <!-- <script src="https://wusfen.github.io/console.js/dist/console.js"></script> -->

</head>

<body>
  <script>
    // 画布
    var canvas = document.createElement('canvas')
    document.body.appendChild(canvas)
    canvas.style.position = 'absolute'
    canvas.style.width = '100%'
    canvas.style.height = '100%'
    canvas.style.color = 'rgb(68, 141, 246)'
    canvas.style.borderColor = 'rgba(255, 255, 255, 0.5)'
    canvas.style.outlineColor = 'rgb(156, 71, 214)'
    var ctx = canvas.getContext('2d')


    // 音频上下文
    AudioContext = window.AudioContext || window.webkitAudioContext
    var audioContext = new AudioContext()
    // 音频源节点
    // var mediaElementAudioSourceNode = new MediaElementAudioSourceNode(audioContext, {mediaElement: document.getElementById('audio')})
    // 音频分析节点
    var analyserNode = audioContext.createAnalyser()
    // 源节点连接分析节点
    // mediaElementAudioSourceNode.connect(analyserNode)
    // 连接扬声器
    // analyserNode.connect(audioContext.destination)

    // 麦克风
    navigator.mediaDevices.getUserMedia({ audio: true })
      .then(function (stream) {
        console.log('getUserMedia', stream)
        // 媒体流源节点
        var mediaStreamAudioSourceNode = audioContext.createMediaStreamSource(stream)
        // // 音频分析节点
        // var analyserNode = audioContext.createAnalyser()
        // 连接分析节点
        mediaStreamAudioSourceNode.connect(analyserNode)
        // // 画图
        // draw(analyserNode)
      })
      .catch(function (e) {
        console.error('getUserMedia', e)
      })

    // 画图
    draw(analyserNode)

    // 循环获取频谱可视化
    function draw(analyserNode) {
      var width = 2048
      // clear
      canvas.width = width
      canvas.height = width

      // 阴影
      ctx.shadowOffsetX = 15
      ctx.shadowOffsetY = 150
      ctx.shadowBlur = 25
      ctx.shadowColor = canvas.style.outlineColor

      // 分析节点取样
      analyserNode.fftSize = width * 2

      // 波形数据
      var array = new Uint8Array(analyserNode.frequencyBinCount)
      analyserNode.getByteTimeDomainData(array)

      ctx.strokeStyle = canvas.style.borderColor
      ctx.beginPath()
      array.forEach(function (y, x) {
        ctx.lineTo(x, width/2 + (y-128)*1)
      })
      ctx.stroke()

      // 频谱数据
      var array = new Uint8Array(analyserNode.frequencyBinCount)
      analyserNode.getByteFrequencyData(array)
      // console.log(array)

      // 一半
      ctx.strokeStyle = canvas.style.color
      ctx.fillStyle = canvas.style.color
      ctx.beginPath()
      // ctx.lineTo(width / 2, 0)
      array.forEach(function (y, x) {
        ctx.lineTo(y + (width / 2) + 0, x)
      })
      // ctx.fill()
      ctx.stroke()

      // 另一半
      ctx.strokeStyle = canvas.style.color
      ctx.fillStyle = canvas.style.color
      ctx.beginPath()
      // ctx.lineTo((width / 2), 0)
      array.forEach(function (y, x) {
        ctx.lineTo((width / 2) - y - 0, x)
      })
      // ctx.fill()
      ctx.stroke()

      // 循环
      setTimeout(() => {
        draw(analyserNode)
      }, draw.timeGap||16)
    }

  </script>
</body>

</html>